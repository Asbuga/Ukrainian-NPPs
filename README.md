# ğŸ“Š Ukrainian NPPs: Open Data Analysis

![Python](https://img.shields.io/badge/python-3.12-blue)
![Open Data](https://img.shields.io/badge/Open--Data-%F0%9F%93%9C-green)
![Nuclear Energy](https://img.shields.io/badge/Nuclear-Energy-yellow)
![Status](https://img.shields.io/badge/status-active-brightgreen)

## Table of Contents

- [About](#about)
- [Examples](#examples)
- [Focus](#-focus)
- [Technologies | Stack](#-technologies--stack)  
- [Data Source](#-data-source)
- [Key Features](#-key-features)
- [Use Case](#-use-case)
- [Quickstart](#-quickstart)
- [Run the notebook](#-run-the-notebook)
- [Envirement](#envirement)
- [Docker](#-docker)
- [Project Structure](#-project-structure)
- [Testing](#-testing)
- [License & Data Usage](#ï¸-license--data-usage)
- [What I Learned / Built Here](#-what-i-learned--built-here)
- [Author](#-author)
- [Development Plans](#ï¸-development-plans)

## About

This project explores open datasets on radioactive emissions and discharges from  
nuclear power plants (NPPs) in Ukraine. Using Python, Pandas, and Plotly, we  
analyze environmental indicators across several stations and visualize quarterly  
changes over time.

### Goal  

better understand trends in emission levels and present the data in an  
accessible, interactive format.  
All data is retrieved from publicly available government sources.

### ğŸ’¡ Why This Project Matters

Ukraine's energy sector, especially nuclear energy, is vital and sensitive.  
However, public data on its status is often fragmented or not easily accessible.

This project makes Ukrainian NPP (Nuclear Power Plant) data:

- ğŸ“‚ **Accessible** â€” parsed and structured from open government sources
- ğŸ” **Usable** â€” cleaned and pre-processed for analysis
- ğŸ” **Reusable** â€” exposes data for other developers, analysts, or watchdogs

By offering an interface (API and visuals in the future), this tool can help:

- Analysts monitor energy safety and trends
- NGOs use this data for transparency reports
- Developers integrate it into educational or civic tech tools

## Examples

![plot-radioactive-release](img\plot-radioactive-release-all-stations.png)

## ğŸ“ˆ Focus  

Data analysis, API integration, interactive visualizations  

## ğŸ§ª Technologies | Stack

- **Backend**: Python, FastAPI, Uvicorn, Nginx
- **Database**: PostgreSQL, SQLAlchemy  
- **Data Analysis**: Pandas, Plotly  
- **Development Tools**: Poetry, Docker, Docker Compose, Ruff, Black  
- **Testing**: Unittest, Pytest  
- **Environment Management**: python-dotenv  
- **Notebooks**: Jupyter Notebook  

## ğŸ“‚ Data Source

- [Open data portal][1]

- [Ecological and radiation situation in the area of â€‹â€‹nuclear power plants][2]

- [How to retrieve a dataset (API)?][3]

[1]: https://data.gov.ua/en/

[2]: https://data.gov.ua/en/dataset/4a9d3d56-bd95-4c3e-97e7-1cdc7bcbd445/resource/d55eebcf-4660-4919-96b3-4894be5a6cda

[3]: https://data.gov.ua/pages/aboutuser2#:~:text=%D0%AF%D0%BA%C2%A0%D0%B7%D0%B0%D0%B1%D1%80%D0%B0%D1%82%D0%B8%20%D0%BD%D0%B0%D0%B1%D1%96%D1%80%20%D0%B4%D0%B0%D0%BD%D0%B8%D1%85%20(API)%3F

## ğŸ” Key Features

- Fetch real-time datasets

- Process Excel reports into structured DataFrames

- Build interactive, filterable visualizations (Plotly)

- Analyze radioactive emissions, discharges, and thresholds

- Modular structure and reusable components

## ğŸ“ˆ Use Case

This project allows researchers and developers to explore environmental metrics  
such as:

- Inert radioactive gas emissions (IRG)

- Iodine radionuclides index

- Long-living isotopes

- Cs-137 and Co-60 emissions/discharges

- Quarterly and annual release indexes

## ğŸš€ Quickstart

Follow these steps to set up and run the project:

### Prerequisites  

- Python 3.12+
- Poetry installed ([Poetry Documentation](https://python-poetry.org/docs/))

### Steps  

1. Clone the repository:

    ```bash
    git clone https://github.com/Asbuga/Ukrainian-NPPs.git
    cd Ukrainian-NPPs
    ```

2. Install dependencies and activate the virtual environment:

    ```bash
    poetry install
    ```

3. Activate your environment with poetry in Bash:

    ```bash
    eval $(poetry env activate)
    ```

    or PowerShell:

    ```powershell
    Invoke-Expression (poetry env activate)
    ```

4. Run the application to verify everything is working:  

    ```bash  
    poetry run python app/main.py
    ```

Now you're ready to explore the project!

## ğŸ““ Run the notebook

To analyze data or visualize results, you can use Jupyter Notebook. Follow these  
steps:

1. Install dependencies and activate the virtual [environment](#-quickstart):

   ```bash
   poetry install
   eval $(poetry env activate)
   ```

2. Run Jupyter Notebook:  

    ```bash
    jupyter notebook
    ```

Now you can explore and analyze the data interactively!

## Envirement

The project uses environment variables to configure the application. Below is a  
list of required variables and their purpose.

### Required Variables

- `DATABASE_URL`: Connection string for the PostgreSQL database.
- `SECRET_KEY`: Secret key for application security.
- `DEBUG`: Enable or disable debug mode (`True` or `False`).

### Example `.env` File  

Create a `.env` file in the `config/` directory with the following content:

```env
DATABASE_URL=postgresql://user:password@localhost:5432/ukrainian_npps
SECRET_KEY=your_secret_key
DEBUG=True
```

**Loading Environment Variables**  
The application automatically loads environment variables from the .env file  
using python-dotenv. Ensure the .env file is correctly configured before running  
the application.

## ğŸ³ Docker

The project can be easily run in Docker containers. Follow these steps to build  
and start the application:

### Steps  

1. Build the Docker images:

    ```bash
    docker compose -f docker/docker-compose.yaml build
    ```

2. Start the services:

    ```bash
    docker compose -f docker/docker-compose.yaml up
    ```

    This will start  the following services:

    - **PostgreSQL**: The database service
    - **FastAPI Application**: The backend application
    - **Nginx**: The reverse proxy server (if configured)

3. Stop the services: To stop and remove the containers, run:

    ```bash
    docker compose -f docker/docker-compose.yaml down
    ```

4. View logs: To view logs for a specific container, use:

    ```bash
    docker logs <container_name>
    ```

**Notes**  
Ensure Docker and Docker Compose are installed on your system.
The docker-compose.yaml file is located in the docker directory.
Environment variables for the database and application are configured in the  
config directory.
Now you can run the project in an isolated Docker environment!

## ğŸ§  Project Structure

```bash
.
â”œâ”€â”€ .github/                     # GitHub Actions configuration for CI/CD
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml               # Script for automating tests and checks
â”‚
â”œâ”€â”€ app/                         # Main application code
â”‚   â”œâ”€â”€ main.py                  # Entry point for running the application
â”‚   â””â”€â”€ core/                    # Core modules of the application
â”‚       â”œâ”€â”€ app.py               # FastAPI configuration
â”‚       â”œâ”€â”€ db.py                # Database logic
â”‚       â”œâ”€â”€ schemas.py           # Data schemas (Pydantic)
â”‚       â”œâ”€â”€ users.py             # User-related logic
â”‚
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ .env                     # Environment variables for the application
â”‚   â””â”€â”€ db.env                   # Environment variables for the database
â”‚
â”œâ”€â”€ docker/                      # Docker files for containerization
â”‚   â”œâ”€â”€ docker-compose.yaml      # Docker Compose configuration
â”‚   â””â”€â”€ Dockerfile               # Instructions for building the Docker image
â”‚
â”œâ”€â”€ notebook/                    # Jupyter Notebook for data analysis
â”‚   â”œâ”€â”€ research.ipynb           # Main notebook for research
â”‚
â”œâ”€â”€ src/                         # Auxiliary modules
â”‚   â”œâ”€â”€ client.py                # API client for external services
â”‚   â”œâ”€â”€ utils.py                 # Utility functions
â”‚
â”œâ”€â”€ tests/                       # Tests for the application
â”‚   â”œâ”€â”€ integration/             # Integration tests
â”‚       â”œâ”€â”€ test_client.py       # Tests for the API client
â”‚       â”œâ”€â”€ test_data_format.py  # Tests for data formatting
â”‚   â””â”€â”€ unit/                    # Unit tests
â”‚
â”œâ”€â”€ .gitignore                   # Files and folders ignored by Git
â”œâ”€â”€ .pre-commit-config.yaml      # Pre-commit hooks configuration
â”œâ”€â”€ poetry.lock                  # Project dependencies (generated by Poetry)
â”œâ”€â”€ pyproject.toml               # Project configuration
â”œâ”€â”€ LICENSE                      # Project license
â””â”€â”€ README.md                    # Project documentation
```

## ğŸ§ª Testing

To ensure the project is working correctly, you can run the test suite. Follow  
these steps:

1. Activate the virtual [environment](#-quickstart).

2. Run the tests using pytest:

    ```bash
    poetry run pytest
    ```

    For a more detailed output, use the verbose mode:

    ```bash
    poetry run pytest -v
    ```

This will execute all unit and integration tests and provide a summary of the  
results.

## âš ï¸ License & Data Usage

All datasets are sourced from data.gov.ua and distributed under the Creative  
Commons Attribution 4.0 License (CC BY 4.0).  
Make sure to attribute the original source if you reuse the data.  

### ğŸ‘¨â€ğŸ’» My Role

This project was created as a part of my learning path and personal interest in  
open data and energy infrastructure in Ukraine.  
My main responsibilities and skills demonstrated here include:

- ğŸ”§ Backend Development: Data parsing, cleaning, transformation using Python  
& Pandas
- ğŸ”Œ API Design (in progress): RESTful endpoints for accessing processed data
- ğŸ“Š Data Analysis: Exploratory data analysis of NPP metrics, preparing it for  
visualization
- ğŸ§± Project Structure: Organizing code for clarity, scalability, and future  
API integration

## ğŸ“š What I Learned / Built Here

- Processing open government data: collection, cleaning, transformation
- Working with tabular sources (CSV, XLSX) using Pandas
- Developing a basic architecture for a REST API (FastAPI, in development)
- Building graphs/visualizations using Plotly
- Organizing the project structure for scaling

## ğŸ‘¨â€ğŸ’» Author

**Andrii Buha**  
Python Backend Developer | Django & FastAPI | Data Processing  

If you're interested in collaboration, feedback, or hiring â€” feel free to  
reach out:

ğŸ“¬ [LinkedIn](https://www.linkedin.com/in/andrii-buha/)  
ğŸ’» [GitHub](https://github.com/Asbuga)  

## ğŸ› ï¸ Development plans  

- [ ] REST API
- [ ] Tests  
- [ ] Deploy  
